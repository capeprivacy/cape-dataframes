{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Cape Python Policy with Pandas and Cape Core\n",
    "\n",
    "This Jupyter Notebook is accompanied by our [Medium Post on Getting Started with Cape Core](https://medium.com/dropoutlabs/cape-core-privacy-and-data-science-working-together-d25a55526506). To follow along, you will need to [download the example dataset](https://capeprivacy.com/example-dataset/) and put it in a relative folder called `data` (or update the file path below). You will also need to [download the policy file](https://github.com/capeprivacy/cape-python/blob/master/examples/policy/iot_example_policy.yaml) and put it in a relative folder called `policy` or ensure you have Cape Python installed locally and change the path to use the copy in the `examples` folder.\n",
    "\n",
    "You will also need a local (or deployed version) of [Cape Core](https://github.com/capeprivacy/cape) running and have generated an API token to follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kjamistan.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7fe9978438>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cape_dataframes as cape_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/iot_example.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+-----------+---------+--------------------+------+------+\n",
      "|          timestamp|       username|temperature|heartrate|               build|latest|  note|\n",
      "+-------------------+---------------+-----------+---------+--------------------+------+------+\n",
      "|2017-01-01T12:18:39|       moonjuan|         26|       76|22989085-e6fe-eae...|     1|   n/a|\n",
      "|2017-01-01T12:22:52|           ylee|         29|       73|ff29e7ab-934f-f7b...|     0|  test|\n",
      "|2017-01-01T12:32:20|    alicecampos|         29|       76|547ed6d5-0e12-4c2...|     0|  test|\n",
      "|2017-01-01T12:36:40|   stevenmiller|         26|       64|e12b053c-d772-c94...|     0|update|\n",
      "|2017-01-01T12:40:26|robinsongabriel|         17|       80|f0bfb52c-b805-cd1...|     1|   n/a|\n",
      "+-------------------+---------------+-----------+---------+--------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(0.1).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Concerns\n",
    "\n",
    "In this dataset which has mock data from wearable devices, we are concerned about the privacy of the individuals. It is a timeseries-based analysis, so we'd like to ensure we retain the ability to see the data of an individual change over time, but we want to provide some basic privacy protections for our exploratory data analysis and later model development activities.\n",
    "\n",
    "The following policy file provides these protections:\n",
    "\n",
    "- [Tokenization](https://docs.capeprivacy.com/libraries/cape-python/transformations/#tokenizer) of the username column with a maximum token length of 10 and a key defined in the file.\n",
    "- [Date Truncation](https://docs.capeprivacy.com/libraries/cape-python/transformations/#date-truncation) for the timestamp column - removing the minutes and seconds of the data but keeping the year, month, date and hour.\n",
    "- [Redaction](https://docs.capeprivacy.com/libraries/cape-python/redactions) of the build column, which reveals information about the device it was built on. In Cape, redaction involves dropping of the matching data so this will change the shape of your dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: iot_dataset_policy\r\n",
      "version: 1\r\n",
      "rules:\r\n",
      "  - match:\r\n",
      "      name: username\r\n",
      "    actions:\r\n",
      "      - transform:\r\n",
      "          type: \"tokenizer\"\r\n",
      "          max_token_len: 10\r\n",
      "          key: \"Please change this :)\"\r\n",
      "  - match:\r\n",
      "      name: timestamp\r\n",
      "    actions:\r\n",
      "      - transform:\r\n",
      "          type: \"date-truncation\"\r\n",
      "          frequency: \"hour\"\r\n",
      "  - match:\r\n",
      "      name: build\r\n",
      "    actions:\r\n",
      "      - transform:\r\n",
      "          type: \"column-redact\"\r\n",
      "          columns: [\"build\"] \r\n"
     ]
    }
   ],
   "source": [
    "!cat ../policy/iot_example_policy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cape Core\n",
    "\n",
    "If you are using Cape Core and have a project setup and registered with the above policy as well as an API token, you can use the following code to download the policy from the Cape Coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cape_df.Client(\"http://localhost:8080\")\n",
    "c.login(\"INSERT YOUR CAPE TOKEN HERE\")\n",
    "policy = c.get_policy(\"first-project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the parsed policy\n",
    "\n",
    "To apply the parsed policy, call the `apply_policy` function to your dataframe and sample the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "caped_df = cape_df.apply_policy(policy, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+---------+------+--------+\n",
      "| timestamp|  username|temperature|heartrate|latest|    note|\n",
      "+----------+----------+-----------+---------+------+--------+\n",
      "|2017-01-01|1763f4313b|         22|       83|     1|  update|\n",
      "|2017-01-01|d0c44f5675|         12|       77|     0|    wake|\n",
      "|2017-01-01|0a89db1e39|         12|       78|     1|interval|\n",
      "|2017-01-01|26594010f3|         29|       76|     0|    test|\n",
      "|2017-01-01|37db75f0f1|         12|       71|     0|   sleep|\n",
      "+----------+----------+-----------+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caped_df.sample(0.1).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send it to Sink\n",
    "\n",
    "Now it's time to send along our caped DataFrame to our clean sink or utilize it in a Spark task (for example, for analytics, EDA or machine learning). \n",
    "\n",
    "Note: You'll need to edit the database details below (or specify where you'd like the dataframe to be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caped_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cape-df",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c0eb9acd3ce9f628738cc91d7613a5d048e1a93f709104c9a35d77254cfaaac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
